[Chatgpt](https://chatgpt.com/?model=auto)

# 옵티마이저 - 경사하강법 & 학습률

> [!NOTE] 선형회귀
- 선형 회귀는 **y = wx + b  형태의 직선 모델**
- 두 변수 간의 관계를 설명하고 예측 가능
- 실전에서는 다중 입력 변수(다중 선형 회귀)에도 적용가능

![[Pasted image 20250423095225.png]]

### 선형 회귀 알고리즘 프로그래밍
1) 문제 정의
2) 데이터 변수의 역할 정의
3) 데이터 모아 보기

### 소프트 벡터 머신 회귀(SVM)

장점:
- 비선형 관계도 모델링 할 수 있음. 이상치에 대해 덜 민감함
단점:
- 데이터 전처리 및 파라미터 튜닝이 필요함
- 널 함수 선택 등의 설정에 따라 성능이 크게 달라질 수 있음

예제)
```python
#Step 1 : 데이터 확인
import pandas as pd
from sklean.model_selection import train_test_split
#abalone.csv 파일을 읽어 'df'에 저장한 후 내용을 출력
df = pd.read_csv('data/abalone.csv')
df.head(3)
#데이터 정보 파악
df.info()
#Step 2 : 데이터 준비하기
# 특성 항목으로 'X'에 'Whole_weight'를, 타겟 항목으로'y'에 'Rings'를 지정한다
x = df[['Whole_weight']].values
y = df['Rings'].values
#학습 데이터와 테스트 데이터를 7:3의 비율로 나눈다
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)
```

### 기계학습 회귀 모델 구현
```python
#Step 1 : 라이브러리 가져오기
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
#Step 2 : 모델 객체 생성하기
model_lr = LinearRegression()
#Step 3 : 모델 학습하기
model_lr.fit(x_train,y_train)
#Step 4 : 모델 예측하기
#1) 모델 예축
y_pred = model_lr.predict(x_test)
y_pred

df_result = pd.DataFrame(colums=['Actual','Predicted'])
df_result['Actual'] = y_test
df_result['Predict'] = y_pred
df_result
#2) 모델 기울기, 절편 확인
print(f'기울기:{model_lr.coef_}')
print(f'절편:{model_lr.intercept_}')
#3)모델로 예측한 최적합선 시각화
plt.figure()
plt.scatter(x_test,y_test,color='gray')
plt.plot(x_test, y_pred,color='red',linewidth=2)

plt.xlabel('Whole_weight')
plt.ylabel('Rings)
plt.show()

#Step5 : 모델 평가하기
print("Linear Regression Score")
print(f'MSE:{mean_squared_error(y_test,y_pred)}')
print(f'MAE:{mean_squard_error(y_test,y_pred)}')

print("R2:", r2_score(y_test,y_pred))
```

# 앙상블 기법
- 여러 개의 분류를 생성하고 그 예측을 결합함으로써 보다 정확한 예측을 도출하는 기법
- 강력한 하나의 모델을 사용하는 대신, 약한 모델 여러 개를 조합하여 더 정확한 예측에 도움을 주는 방식

# 배깅(Bagging, Bootstrap aggreagting)
- 주어진 데이터셋에서 무작위 샘플링을 통해 여러 데이터 서브셋을 구성하고, 각각의 서브셋에서 독립적으로 model을 학습시킨 뒤, 모든 모델의 예측을 집계(voting)하는 기법
## 배깅 - Voting
- 서로 다른 알고리즘을 가진 분류기 중 투표를 통해 최종 예측 결과를 결정하는 방식
- Hard voting : 각 weak learner들의 예측 결과 값을 바탕으로 다수결에 투표하는 방식
- Soft voting : 각 weak learner들의 예측 확률값의 평균 또는 가중치 합을 사용

### 회귀모델 예제(보험청구 예측)
```python

```

## 선형회귀 
- 모델: 기본적인 선형회귀 모델
- 목표: 독립변수(x)와 종속 변수(y) 사이의 직선 관계를 학습
- 단순, 빠름, 해석하기 쉬움
- 과적합 가능성 높음
- 피처가 적고 다중공선성 문제가 없을 때 사용하면 좋음
```python
#선형회귀
from sklearn.linear_model import LinearRegression
from sklearn import metrics
#객체 생성
model_lr = LinearRegression()
#학습하기
model_lr.fit(x_train, y_train)
#예측하기
print(model_lr.intercept_)
print(model_lr.coef_)
print(model_lr.score(x_test, y_test))
#컬럼명과 계수를 쌍으로 묶기
coef_df = pd.DataFrame({
	'Feature': x_train.columns,
	'Coefficient' : model_lr.coef_
})
#계수를 기준으로 정렬(절댓값 기줒ㄴ으로 내림차순 정렬)
coef_df = coef_df.reindex(coef_df['Coefficient'].abs().sort_values(ascending=False).index)
#정렬된 계수를 출력
print(coef_df)
print(f'Lin_reg.score:{model_lr.score(x_test,y_test)}')


import matplotlib.pyplot as plt
import numpy as np
from sklearn import metrics

  

# Predictions from the RandomForestRegressor

x_train_pred = model_lr.predict(x_train)
x_test_pred = model_lr.predict(x_test)

  

# Mean squared error and R2 scores

print('MSE train data: %.3f, MSE test data: %.3f' % (metrics.mean_squared_error(x_train_pred, y_train), metrics.mean_squared_error(x_test_pred, y_test)))

print('R2 train data: %.3f, R2 test data: %.3f' % (metrics.r2_score(y_train, x_train_pred), metrics.r2_score(y_test, x_test_pred)))

  

# Plot

plt.figure(figsize=(8,6))

# Plot training data

plt.scatter(x_train_pred, y_train - x_train_pred, c='grey', label='Train data', alpha=0.5)

# Plot test data

plt.scatter(x_test_pred, y_test - x_test_pred, c='blue', label='Test data')

# Plot horizontal line for error=0

plt.axhline(y=0, color='r', linestyle='-')

# Labels and title

plt.xlabel('Predicted values')
plt.ylabel('Actual values')
plt.title('Predicted vs Actual Values')
plt.legend()
plt.show()


import matplotlib.pyplot as plt

# 중요 변수를 막대 그래프로 시각화

plt.figure(figsize=(10, 6))
plt.barh(coef_df['Feature'], coef_df['Coefficient'], color='skyblue')  # 가로 막대 그래프(barh)

plt.xlabel('Coefficient Value')  # x축 레이블
plt.ylabel('Feature')  # y축 레이블
plt.title('Feature Importance (Linear Regression)')  # 그래프 제목
plt.gca().invert_yaxis()  # 큰 값이 위에 오도록 y축 반전
plt.show()
```

## 릿지회귀
- L2 정규화를 사용하여 모델을 학습. 즉, 회귀 계수에 패널티를 부여함
- 손실 함수에 가중치 계수의 제곱합을 추가해 과적합을 방지
- 모델이 복잡한 경우 과적합을 줄여 안정적인 성능을 냄
- 모든 피처를 사용하지만 회귀 계수의 크기를 줄인다
- 피처가 많고 과적합이 우려될 때 사용하면 좋음
```python
# 릿지회귀(Ridge)
from sklearn.linear_model import Ridge

# 객체 생성
model_ridge = Ridge(alpha=0.5)

# 학습하기
model_ridge.fit(x_train,y_train)   # ... 코드 입력

print(model_ridge.intercept_)
print(model_ridge.coef_)
print(model_ridge.score(x_test, y_test))

# 컬럼명과 계수를 쌍으로 묶기
coef_df = pd.DataFrame({

    'Feature': x_train.columns,  # 각 피처 이름

    'Coefficient': model_ridge.coef_ # 각 피처의 계수

})

# 계수를 기준으로 정렬 (절댓값 기준으로 내림차순 정렬)

coef_df = coef_df.reindex(coef_df['Coefficient'].abs().sort_values(ascending=False).index)
# 정렬된 계수를 출력
print(coef_df)
print(model_ridge.score(x_test, y_test))
```

## 라쏘회귀
- Lasso는 스케일에 민감하므로 **정규화(Scaling)가 필수적
- L1 정규화를 사용하여 일부 회귀 계수를 0으로 만든다
- 이로 인해 변수 선택(feature)
- 